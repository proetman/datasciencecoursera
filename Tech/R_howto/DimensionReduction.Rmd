---
title: "Dimension Reduction Howto"
author: "Paul roetman"
date: "11 July 2017"
output: 
  html_document:
    toc: TRUE
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Dimension Reduction

```{r}
set.seed(12345)
par(mar=rep(0.2, 4)) 
dataMatrix <- matrix(rnorm(400), nrow=40)
image(1:10, 1:40, t(dataMatrix)[,nrow(dataMatrix):1])

```

Note: no pattern emerges, as there is no pattern to the data!
```{r}
heatmap(dataMatrix)
```
Add Pattern, can see that the left half is more red, right more yellow
on right, mean of 3
on left, mean of 0

```{r}
set.seed(678910)
for (i in 1:40) {
        # flip a coin
        coinFlip <- rbinom(1, size=1, prob=0.5)
        # if heads, add a common pattern to that row
        if (coinFlip){
                dataMatrix[i, ] <- dataMatrix[i,] + rep(c(0,3), each = 5)
        }
}
par(mar=rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

Now run hierarchical analysis
can see definate clusters on right/left, this shows in heat map.
in rows, not so much, still pretty random.

```{r}

par(mar=rep(0.2, 4)) 
heatmap(dataMatrix)
```

## Patterns in rows and columns

```{r}
hh <- hclust(dist(dataMatrix))
hh
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, , xlab="Row Mean", ylab="row", pch=19)
plot(colMeans(dataMatrixOrdered), xlab="Column", ylab="ColMeans", pch=19)
```

## Related Problems

You have multivariate variables X1, X2, ... , Xn. To simplify, X1 = (X11, X27, X321), X2 = ...

* find new set of multivariate variables that are uncorrolated, but still explain as much variance as possible (basically same as orig). For example, height and weight are corrolated, so they can probably be lumped together; whereas height and hair colour cannot.

* if put all variables together in one matrix, find best matrix created with fewer variables (lower rank) that explains the original data.

First goal is _statistical_ (often solved by principle componants analysis).
Second goal is _data compression_ (often solved by singular value decomposition)

## SVD - singular value decomposition

If X is matrix, each var in col, each obs in row, then SVD is matrix decomposition.
        X = UDV^T^
  where the col of U are orthoganl (left singular vectors)
  the col of V are orthogonal (right singular vectors)
  and D is a diagonal matrix (singular values)
  
  
## PCA - principle componants analysis

The principal components are equal to the right singular values if you first scale (subtract mean, divide by standard deviation) the variables; then run SVD on the result, end up with V.
        
Components of SVD - _u_ and _v_

First row of _U_ represents the group of data that divides into groups the highest. So in this case the first value of $d is 12, so the first row of _U_ is the clumping of the data for that 12 (this is not 12% of all values changed).

Same applies for $v

So, SVD picks up the pattern automagically.

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd1$u[,1], 40:1, , xlab="Row", ylab="First Left Singular vector", pch=19)
plot(svd1$v[,1], xlab="Column", ylab="First Right Singular vector", pch=19)

```
Have a look at the raw data.
```{r}
svd(scale(dataMatrixOrdered))
```

### Components of SVD - Variance Explained

$d contains the Variance Explained (diag matrix), it is the Percent of variation that is explained by that component. Components ordered with first being the largest.

first plot is: raw singular value, not much meaning as its an interpretable scale???? The 12 does not really mean 12%.
second plot is: divide by total sum of all values, gives the "correct" proportion, ie 40%

```{r}
par(mfrow = c(1,2))
plot(svd1$d, xlab = "column", ylab="Singular value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="Prop. of variance explained", pch=19)
```

### To show svd is close to principle components

So: svd and pca are basically identical. One uses function SVD, other uses function PRCOMP.

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale=TRUE)
plot(pca1$rotation[,1], svd1$v[,1], pch=19, xlab="Principle Component 1", ylab="Right Singular Vector 1")
abline(c(0,1))
```

### Components of SVD - Variance explained

Very simple example, matrix has either 0's (first 5 col) or 1's (second 5 col)

first singular value explains 100% of variance (3^rd^ graph).


```{r}
constantMatrix <- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0,1), each=5)}
svd1 <- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d, xlab="Column", ylab="Singular Value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="Prop. of variance explained", pch=19)
```


Now add 2^nd^ pattern.

first pattern will set data in columns - first half data with one mean, 2nd half data have another.
Second pattern will set data in rows - alternate between columns

The first set of graphs displays what the patterns are:

```{r}
set.seed(678910)
for (i in 1:40){
        # flip coin
        coinFlip1 <- rbinom(1, size=1, prob=0.5)
        coinFlip2 <- rbinom(1, size=1, prob=0.5)
        
        # if coin is heads, add common pattern to row
        if(coinFlip1) {
                dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5), each=5)
        }

        if(coinFlip2) {
                dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5), 5)
        }
        
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]

svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0,1), each=5), pch=19, xlab="Column", ylab="Pattern 1")
plot(rep(c(0,1), 5), pch=19, xlab="Column", ylab="Pattern 2")

```

Now, for this example, we now how the data was generated (ie we know the truth), but that doesnot happen in reality. Need to figure out what the "truth" is.

Need algorith that can pick up on the two sets of data. That is what the SVD is for.

Can see in the following graphs.

First vector picks up the block pattern, first 5 are somewhat lower, 2nd 5 higher
Second vector tries to pick up the alternating mean pattern. Every other point is higher or lower.
Not as obvious as when plotting the "truth", but does a pretty good job!

```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered) [, nrow(dataMatrixOrdered):1])
plot(svd2$v[,1], xlab="Column", ylab="First right singular Vector", pch=19)
plot(svd2$v[,2], xlab="Column", ylab="Second right singular Vector", pch=19)

```

now look at variance explained

first component explains over 50% of the variation. Second about 18%.

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,2))

plot(svd1$d, xlab="Column", ylab="Singular value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab="Column", ylab="percent of variance explained", pch=19)
```

## Missing Values

```{r}
dataMatrix2 <- dataMatrixOrdered
### Randomly insert missing data

dataMatrix2 [sample(1:100, size=40, replace=FALSE)] <- NA

# Next command will generate an error
tryCatch({
  svd1 <- svd(scale(dataMatrix2))  # Will fail.      
}, warning = function(war) {
        print(paste("WARNING RAISED:", war))
}, error = function(err) {
        print(paste("ERROR RAISED:", err))
})
# 
# Error: infinite or missing values in 'x'

```


### Imputing {impute}

Note: only real difference is in the bottom left corner, values for "1" are a little different.

knn: takes a missing row, and imputes it by looking at the "k" nearest values for that row. Default 5 (I think), so averages the 5 values.

No major effect on output.

```{r}
# To install
# source("https://bioconductor.org/biocLite.R")
# biocLite("impute")
library(impute)
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered))
svd2 <- svd(scale(dataMatrix2))

par(mfrow=c(1,2))
plot(svd1$v[,1], pch=19)
plot(svd2$v[,1], pch=19)
```

## FACE Example
See notes for face image reduction


This is an example of data compression. ie only store the first 5 or 10 variances.

Data Compression and Statistical Summary are 2 sides of the same coin.

Summarise data with smaller number of features, SVD can be used.


# Summary

* Scale matters
* PC's / SV's may mix real patterns
* can be computationally intensive
* Advanced data analysis from an elementrary point of view: http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf
* Elements of statistical leaning: http://www-stat.stanford.edu/~tibs/ElemStatLearn/
* Alternatives (different methods, but similar)
** Factor Analysis:  http://en.wikipedia.org/wiki/Factor_analysis
** Independent component analysis: http://en.wikipedia.org/wiki/Independent_component_analysis
** Latent semantic analysisL http://en.wikipedia.org/wiki/Latent_semantic_analysis







